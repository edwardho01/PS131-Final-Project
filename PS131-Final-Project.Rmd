---
title: "Final Project"
author: "Edward Ho"
date: "`r Sys.Date()`"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Introduction 

My project will outline predictor variables of interest that could potentially predict the severity of a car accident.

### What's in this dataset? What is it?

To begin, the dataset is a countrywide record of car accidents from 49 different states. The data set covers a wide time frame where it tracks monthly and daily data spanning from 2016 to 2021. It's important to note that the data was collected from various sourcesâ€”such as the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors, etc. There's a wide variety of data, with over 1.5 accidents, or observations, that have been recorded with the highest frequencies of recorded accidents being from the state of California. This model contain various variables, but most notably, one variable contained is the severity attribute which spans from 1-4, where 1 is the lowest severity and 4 is the highest. This severity is determined by it's impact on traffic, or the shortest delay on oncoming traffic as a result of the accident.

### Why even create the model?

I'll list further details on the variables below, but essentially, the original data set contained nearly 47 different variables. All these variables levels in importance based on what one might be interested in predicting, but as I noted previously, I want to see what factors may be especially important in predicting severity of a certain accident. Many times we are forced to drive in various different weather conditions and usually we bat a blind eye towards the dangers that may result from these conditions. Weather conditions such as wind speed, weather conditions (rain, snow, etc.), night/day, temperature, and maybe even the presence of certain road factors (roundabouts, cross ways, etc.). It may be important to know the relationship of these accidents and how they might vary in severity to potentially raise awareness to future drivers.

### Loading Data and Packages

The project was found through Kaggle, and as I noted before, there were nearly 1.5 million observations in the data. Therefore, I wasn't able to import the raw data as a whole into R, therefore I must note that I did some light preliminary cleaning of the data set by filtering by only what I will be using as the subject of my analysis, 2021 California data. Therefore, after filtering out blank inputs, and grouping by 2021 and California exclusive cases, we now have a new data set that I can do further cleaning on. Let's clean the data a bit before displaying the full table and splitting the data for further analysis.

```{r message=FALSE, warning=FALSE, include=FALSE}
library(readr)
library(tidymodels)
library(tidyverse)
library(ISLR)
library(ISLR2)
library(discrim)
library(janitor)
library(poissonreg)
library(pROC)
library(corrplot)
library(purrr)
library(caret)
library(dplyr)
library(binom)
library(themis)
library(randomForest)
library(parsnip)
library(rpart)
library(rpart.plot)
library(ranger)
library(vip)
library(xgboost)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
accidents_data <- read_csv("D:/UCSB/PSTAT131/Final Project/data/accidents.csv")
```

### Data Cleaning

Below, we will be removing some unneeded variables, renaming some columns, and other techniques for needed clarity in reading the data. Note, there is no need to remove missing/blank observations since I have already made sure they were not included through excel and before importing the csv.

* Check for any remaining na's

```{r}
accidents <- na.omit(accidents_data)
sum(is.na(accidents))
```


* Remove the start_time and state column since I had already filtered that completely in excel. variables

```{r}
accidents <- accidents %>% 
  select(-Start_Time,-State,-Wind_Direction)
```

* Cleaning the variable names for clarity

```{r}
accidents <- janitor::clean_names(accidents)
```

* Now I must factor encode all the variables that are boolean values.

```{r}
accidents$severity <- factor(accidents$severity)
accidents$side <- factor(accidents$side)
accidents$weather_condition <- factor(accidents$weather_condition)
accidents$bump <- factor(accidents$bump)
accidents$crossing <- factor(accidents$crossing)
accidents$railway <- factor(accidents$railway)
accidents$stop <- factor(accidents$stop)
accidents$traffic_signal <- factor(accidents$traffic_signal)
accidents$sunrise_sunset <- factor(accidents$sunrise_sunset)
```

Now the data is cleaned, and we can proceed with the analysis

### Variables
Below are noteworthy variables that will be used in our analysis (note that there are 17 variables, but I chose only select few important ones to display here. A full, detailed explanation of each variable can be found in the codebook in the zip file.)

> **Table 1:** Variable Details

Variable Name | Description | Type
---|---|---
severity | Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay). | Numeric
side | Shows the relative side of the street (Right/Left) | Factor
visbility_mi | 	Shows visibility (in miles). | Numeric
wind_speed_mph | Shows wind speed (in miles per hour). | Numeric
precipitation_in | 	Shows precipitation amount in inches, if there is any. | Numeric
weather_condition | 	Shows the weather condition (rain, snow, thunderstorm, fog, etc.) | Factor
crossing | Presence of crossing in a nearby location. | Factor
stop | Presence of stop in a nearby location. | Factor
traffic_signal | Presence of traffic signal in a nearby location. | Factor
sunrise_sunset | hows the period of day (i.e. day or night) based on sunrise/sunset. | Factor

### Data Split

I split my data using a proportion of 80/20, where the training data is 80% of my total data, and the testing data covers the other 20%. Because I had a large imbalance in the response variable, severity, then I must use stratified sampling (which you can see in the EDA portion).

```{r}
accidents_split <- initial_split(accidents, prop = 0.80,strata = severity)
accidents_train <- training(accidents_split)
accidents_test <- testing(accidents_split)
```

Now I run a sanity check on the dimensions of the testing and training set to see if they match

```{r}
dim(accidents)
dim(accidents_train) + dim(accidents_test)
```

## Exploratory Data Analysis

The proceeding section will be exploratory data analysis on the training set to find any potential patterns and notions we might have before we proceed with fitting the models.

### Severity Distribution

```{r}
ggplot(accidents_train, aes(severity)) +
  geom_bar() +
  labs(
    title = "Severity Distribution",
    x = "Severity",
    y = "Count"
  )
```

Here is the proportion of the response variable with a severity of 4.

```{r}
sum(accidents_train$severity=="4")/nrow(accidents_train)
```

Notice that we find ourselves an extremely prevalent class unbalance in our response observations, therefore, something such as down sampling will be extremely valuable later on when creating the recipe. Also, notice how the response variables only contain 2 and 4 despite the range being from 1-4, therefore it appears all the accidents were either mild or severe in terms of severity.

### Exploring the Location landmarks Factor Variables

```{r}
value <- c("TRUE" , "FALSE")
amount <- c(sum(accidents_train[12]=="TRUE"),sum(accidents_train[12]=="FALSE"),sum(accidents_train[13]=="TRUE"),sum(accidents_train[13]=="FALSE"),sum(accidents_train[14]=="TRUE"),sum(accidents_train[14]=="FALSE"),sum(accidents_train[15]=="TRUE"),sum(accidents_train[15]=="FALSE"),sum(accidents_train[16]=="TRUE"),sum(accidents_train[16]=="FALSE"))
condition <-c("bump","bump","crossing","crossing","railway","railway","stop","stop","traffic_signal","traffic_signal")
data <- data.frame(value,condition,amount)
ggplot(data, aes(x = condition, y = amount, fill = value)) +
  geom_col(position = "fill")
```

The first thing we can notice already is the that crossing, traffic signals, and stop related traffic incidents are much more frequent than bumps and railway accidents.

### Potential Predictor Correlation

```{r}
accidents_train %>%
  select(where(is.numeric)) %>%
  cor() %>%
  corrplot(method = 'color',type = "lower",diag = FALSE,addCoef.col="Black")
```


### Weather conditions

```{r}
ggplot(data = accidents_train) + geom_bar(mapping = aes(x = weather_condition)) + coord_flip()
```

### Severity and Weather Condition

```{r}
severity_2_dist <- accidents_train %>%
    group_by(weather_condition) %>%
    filter(severity==2) %>%
    summarise(count = n()) %>%
    ggplot() +
    geom_density(mapping = aes(x=count)) +
    scale_x_log10() + 
    labs(x = "Weather condition", y="Density", title ="Density of severity 2 acccidents by weather condition")

severity_4_dist <- accidents_train %>%
    group_by(weather_condition) %>%
    filter(severity==4) %>%
    summarise(count = n()) %>%
    ggplot() +
    geom_density(mapping = aes(x=count))+
    scale_x_log10() + 
    labs(x = "Weather condition", y="Density", title ="Density of severity 4 acccidents by weather condition")

print(severity_2_dist)
print(severity_4_dist)
```

Essentially, what this graph is telling is that as the amount of different weather conditions increase in our data set, there are fewer and fewer severity 2 accidents. But vice versa, there are few weather conditions that have a high number of severity 2 accidents.

```{r}
severity_2 <- accidents_train %>%
    group_by(weather_condition) %>%
    filter(severity==2) %>%
    summarise(count = n()/nrow(.)*100) %>%
    arrange(-count) -> severity2_weather
severity_2
```

```{r}
severity_2_plot <- severity2_weather %>%
    ggplot() +
    geom_col(mapping=aes(x=reorder(weather_condition, -count), y=count, fill=weather_condition))+ 
    theme(text = element_text(size=10)) +
    labs(x = "Weather condition", y="%", title ="Severity 2 acccidents by weather condition") +
    coord_flip()
severity_2_plot
```

```{r}
severity_4 <- accidents_train %>%
    group_by(weather_condition) %>%
    filter(severity==4) %>%
    summarise(count = n()/nrow(.)*100) %>%
    arrange(-count)
severity_4
```

```{r}
severity_4_plot <- severity_4 %>%
    ggplot() +
    geom_col(mapping=aes(x=reorder(weather_condition, -count), y=count, fill=weather_condition))+ 
    theme(text = element_text(size=10)) +
    labs(x = "Weather condition", y="%", title ="Severity 4 acccidents by weather condition") +
    coord_flip()
severity_4_plot
```

## Model Building

## Recipe, Folds, and Saving the Data

```{r}
accidents_folds <- vfold_cv(accidents_train, v=5, strata=severity)

accidents_recipe <- recipe(severity ~ ., data = accidents_train) %>%
  step_dummy(side) %>%
  step_dummy(weather_condition) %>%
  step_dummy(bump) %>%
  step_dummy(crossing) %>%
  step_dummy(railway) %>%
  step_dummy(stop) %>%
  step_dummy(traffic_signal) %>%
  step_dummy(sunrise_sunset) %>%
  step_normalize(all_predictors()) %>%
  step_upsample(severity,over_ratio = 1)
```

### Preparing and running the Models

The models I'll be using are:

* logistic regression
* single decision tree
* boosted tree
* random forests
* k-nearest neighbors

## logistic regression

```{r}
log_reg <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

log_wkflow <- workflow() %>%
  add_model(log_reg) %>%
  add_recipe(accidents_recipe)

log_fit <- fit(log_wkflow,accidents_train)
log_fit %>%
  tidy()
```


## single decision tree

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("classification")

class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% 
  set_args(cost_complexity = tune())) %>%
  add_recipe(accidents_recipe)

class_tree_param_grid <- grid_regular(cost_complexity(range = c(-10, -1)), levels = 2)

class_tree_tune_res <- tune_grid(
  class_tree_wf, 
  resamples = accidents_folds, 
  grid = class_tree_param_grid, 
  metrics = metric_set(roc_auc)
)
autoplot(class_tree_tune_res)
```

## random forests

```{r}
spec <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") 
rf_spec <- spec %>%
  set_mode("classification")
rf_spec_wf <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = tune(),trees = tune(), min_n = tune())) %>%
  add_recipe(accidents_recipe)
rf_param_grid <- grid_regular(mtry(range = c(1,16)),trees(range = c(64,128)),min_n(range = c(1,10)),levels = c(2,2,2))
rf_tune_res <- tune_grid(
  rf_spec_wf, 
  resamples = accidents_folds, 
  grid = rf_param_grid, 
  metrics = metric_set(roc_auc)
)
autoplot(rf_tune_res)
```



## boosted tree

```{r}
```


## k-nearest neighbors

```{r}

```













